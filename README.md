
# Google Cloud & YouTube-8M Video Understanding Challenge
[https://www.kaggle.com/c/youtube8m]

# Data Playground (Predictive Analytics Project Capstone) 

Here we will apply data mining techniques in a real world case study. The case study concerns nash equilibrium in personal fitness context (walking), but the principles apply equally to any personal assessment involving individual-level correction. This repo is really a "lab" for practically testing our skills in a real world context. Facility with R or Python will be uncovered, and some familiarity with predictive modeling and time series analysis.

> Note:  This project is about personal data study and uplift modeling. 
> The data in the course are Ihar Rubanau GPS coordinates collected by
> smartphone in period ~2 years , as the domain is relatively standard
> and likely to be familiar to most of you, hence the case is ideal as a
> real-world case study for analytics and machine learning and to who
> need to be prepared to apply their analytical skills in everyday life
> situations.

[Data Science solutions Playground version 1.0]

 - Data Mining 
 - Anomaly Detection Deep learning 
 - Persuasion Analytics
 - Interactive Data Visualization
 - Meta Analysis
 - Introduction to Design of Experiments
 - Discrete Choice Modeling and Conjoint Analysis
 - Forecasting Analytics
 - Survival Analysis

# Data Mining (R) 

In this part, “Data Mining (R),” we will implement partitioning data and use a holdout sample, measure the performance of predictive models, and what to do about the problem of overfitting.  Popular classification methods (logistic regression, k-nearest-neighbors, classification trees) and prediction methods (linear regression and regression trees) will be discussed.  Collaborative filtering and association rules are also under investigation :)


**1: Getting Started**
Data preparation
Data partitioning (holdout data)
Measuring the performance of classification and prediction models
K-nearest neighbors classification

**2: Linear Regression and CART**
Multiple linear regression
Classification and regression trees

**3:  Logistic Regression**
Propensities and ranking

**4: Recommender Systems**
Association Rules - Apriori Algorithm
Collaborative Filtering - k-Nearest neighbors

# Anomaly Detection
In this part, we will examine data with the goal of detecting anomalies or abnormal instances. This task is critical in a wide range of applications ranging from fraud detection to surveillance. At the end we will need understood the different aspects that affect how this problem can be formulated, the techniques applicable for each formulation and knowledge of some real-world applications in which they are most effective.

**1: Getting started**
The different aspects of anomalies
Classification-based approaches

**2: Unsupervised approaches**
Clustering
Nearest-neighbour
Other statistical techniques

**3: Non-standard approaches**
Information-theoretic methods
Spectral techniques

**4: Applications**
Credit-card fraud
Intrusion detection
Insurance
Healthcare
Surveillance
 
# Deep learning
In this part, we will use emergent rapidly evolving techniques of Deep Learning. The surge in deployed applications based on concepts and methods in this field is an indication of its potential to help fully realize the promise of Artificial Intelligence. At the end of this part we will test the basic concepts underlying the representations and methods in deep learning and see some applications where deep learning is most effective. We will also gain an appreciation of what kind of problems are most suited for this field and current research trends.

**1: Neural Networks and Optimization**
Overview
Machine Learning Basics
Deep Feedforward Networks
Basic Optimization Algorithms

**2: Convolutional Networks and Image Processing**
Convolutional Networks
Practical Methodology
Image Applications
 
**3: Recurrent Architectures and Language Processing**
Recurrent Networks
Long Short Term Memory
Language Applications

**4: Advanced Topics, Research Trends**
Autoencoders
Representation Learning
Deep Generative Models

![enter image description here](http://www.statistics.com/uploads/images/diagram-deep-learning.png)
Deep learning and its place in the learning hierarchy

# Persuasion Analytics 

In this part, about persuasion analytics we will apply predictive modeling methods, and persuasion (uplift) models in particular.  The focus will be on targeting potential solutions to diversificate walking expirience. We will understand which aspects of context are especially important for modeling purposes, and the difference between traditional targeting methods and more recent micro-targeting techniques. The course covers what to measure and how to design user - AI communication  to measure it.  The role of experiments is discussed, and we will try social media fit in.

**1:  Background and basic context concepts**
Why context need to be classified (layered)
Phases of a context layers
Finding the right baggage for the right layer phase
Calculating the effectiveness of a compactification
 
**2: Traditional scenarios vs. individual level modeling and beginning the modeling process**
Traditional scenarios
Microtargeting - shifting the focus to the individual
How to avoid deciding what to predict
Communication instrument design
The modeling process
 
**3: The modeling process in detail**
Common pitfalls
Missing values
Building new indicators
Evaluating models
Combining models
 
**4: Persuasion (uplift) modeling**
Controlled and natural experiments
Combining A-B test with predictive modeling
Persuasion:  determining for whom the message works
Targeting for chaotisation
Targeting for ordering

# Interactive Data Visualization

In this part, we will concentrate on the interactive exploration of data, and how it is achieved using state-of-the-art data visualization software. Explore a range of different data types and structures. Various interactive techniques for manipulating and examining the data and producing effective visualizations.

Mostly exploration of quantitative business data to discern meaningful patterns, trends, relationships, and exceptions that reveal business performance, potential problems and opportunities.

**1: Information visualization characterization and history**
Elements of visual perception
Software introduction and data preparation (merging data, getting started, export)

**2: Interaction techniques**
Distribution analysis
Hands-on visual exploration of business data

**3: Time Series**
Multivariate views (scatterplots, parallel coordinate plots, trellising)
Treemaps for hierarchical data

**4: Specialized visualizations**
Video demonstrations of novel techniques
From visualization to visual analytics

# Meta Analysis (R)

Meta analysis, the ‘analysis of analyses’, is the term used to describe the quantitative synthesis of scientific evidence. The aim of this part is to use the fundamentals of meta-analysis in the R language. Fundamentals of the fixed and random effects models for meta-analysis, the assessment of heterogeneity, and evaluating bias. Handling of rare events, missing data, and indirect treatment comparisons, among other topics.

**1:  Meta Analysis**
Reference Management
Data Preparation for Meta-Analysis

**2: Types and Models for Effect Sizes**
Outcomes in Meta-Analysis
Types of Effect
Fixed Effects Model
Random Effects Model
Reporting, Forest Plots, and Interpretation

**3: Bias, Heterogeneity, and Meta-Regression**
Bias
Evaluating and Reporting Bias
Heterogeneity
Assessing and Reporting Heterogeneity
Meta-regression

**4: Advanced Topics**
Missing Data
Individual Patient Data Meta-Analysis
Rare Events and Small Studies
Network Meta-Analysis

**5: Effect Sizes in Meta Analysis**
Refresher of basic issues in meta-analysis
Computing effect sizes
Fixed and random-effects models

**6: Subgroups in Meta Analysis**
Subgroups analysis
Meta-regression

**7: Multiple Groups and Outcomes**
Working with multiple independent subgroups within studies
Working with multiple outcomes within studies

**8: Cumulative and Sensitivity Analysis**
Publication bias
Cumulative analysis
Sensitivity analysis

# Introduction to Design of Experiments

Design of Experiments is about to gain maximum knowledge at minimum cost (Knowledge mining). For processes of any kind that have measurable inputs and outputs, Design of Experiments (DOE) methods guide you in the optimum selection of inputs for experiments, and in the analysis of results.

**1: Foundations**
Measure of quality (Cp Cpk, dpm)
DOE key concepts
Interactions
Coding
Confounding/aliasing
Robustness
Randomization

**2: Simple Designs and Their Analysis**
DOE 12-step checklist example
Calculating effects
Interaction plots
Marginal means plot of effects
Pareto chart of effects
Prediction equations

**3: Design Types**
Full factorial designs
Fractional factorial designs
Design resolution
Aliasing pattern
Fold-over
Plackett-Burman designs
Box-Behnken designs
Box-Wilson (central composite) designs
Taguchi designs

**4: Practice Conducting and Analyzing Experimental Data**
Multiple regression
Normal probability plot
Importance of analyzing interactions
Taguchi's signal to noise ratios
Variance reduction analysis
Practice planning, executing, and analyzing an experiment

# Discrete Choice Modeling and Conjoint Analysis

In pushing notifications what attributes matter to user? Voice? Maximum information? Average frequency? Uptime? What is the гыук's ideal assesment? Are there differential spectrum of solutions could be constructed?

Statistical techniques that address questions like this. Conjoint analysis is a marketing research technique that asks respondents to rank, rate, or choose among multiple points, where each service is described using multiple characteristics. We will use experimental designs to manipulate the appearance of attribute levels in interaction concepts. Then, after the data will be collected, We will use statistical methods to infer how the attribute levels drive preference or choice. At the end resulting model to model how the users would choose among a set of competing scenarios alternatives.

**1: Fundamental Concepts**
Ranks, ratings, choices
Random utility models

**2: Designing Conjoint and Choice Studies**
Samples
Criteria for obtaining better designs

**3: Conjoint Analysis of Rank Data**
Conjoint analysis of rank and ratings
Conjoint simulation

**4: Introduction to the Multinomial Logit Model**
Multinomial logit model
Maximum likelihood estimation
Extensions to the basic model

# Forecasting Analytics

Choosing an appropriate time series forecasting method, fit the model, evaluate its performance, and use it for forecasting. We will focus on the most popular business forecasting methods: Regression models, smoothing methods including Moving Average (MA) and Exponential Smoothing, and Autoregressive (AR) models. And enhancements such as second-layer models and ensembles, and various issues encountered in practice.

**1: Characterizing Time Series and the Forecasting Goal; Evaluating Predictive Accuracy and Data Partitioning**
Visualizing time series
Time series components
Forecasting vs. explanation
Performance evaluation
Naive forecasts

**2: Smoothing-Based Methods**
Model-driven vs. data-driven methods
Centered and training Moving Average (MA)
Exponential Smoothing (simple, double, triple)
De-trending and seasonal adjustment
Differencing
 

**3: Regression-Based Models**
Overview of forecasting methods
Capturing trend seasonality and irregular patterns with linear regression
Measuring and interpreting autocorrelation
Evaluating predictability and the Random Walk
Second-layer models using Autoregressive (AR) models

**4: Forecasting in Practice**
Forecasting implementation issues (automation, managerial forecast adjustments, and more)
Communicating forecasts to stakeholders
Overview of further forecasting methods (neural nets, ARIMA, and logistic regression)
Forecasting binary outcomes

# Survival Analysis

Various methods used for modeling and evaluating survival data, also called time-to-event data. We will use survival models for traffic avoid and health correction. Survival analysis also has been applied to the field of engineering, where it typically is referred to as reliability analysis.

General statistical concepts and methods include survival and hazard functions, Kaplan-Meier graphs, log-rank and related tests, Cox proportional hazards model, and the extended Cox model for time-varying covariates.

**1: Overview**
Censoring
Key terms: survival and hazard functions
Goals of a survival analysis
Data layout for the computer
Data layout for understanding
Descriptive statistics for survival analysis- the hazard ratio
Graphing survival data- Kaplan Meier
The Log Rank and related tests.

**2: Introduction to the Cox Proportional Hazards (PH) model- computer example**
Model definition and features
Maximum likelihood estimation for the Cox PH model
Computing the hazard ratio in the Cox PH model
The PH assumption
Adjusted survival curves
Checking the proportional hazard assumption
The likelihood function for the Cox PH model

**3: Introduction to the Stratified Cox procedure**
The no-interaction Stratified Cox model
The Stratified Cox model that allows for interaction

**4: Advanced**
Definition and examples of time-dependent variables
Definition and features of the extended Cox model
Stanford Heart Transplant Study Example
Addicts Dataset Example
The likelihood function for the extended Cox model.


**Machine Learning implementation example** 
(incomplete personal GPS Data from last 2 years)

DataExampleUno []

[RawData](https://http://bit.ly/2hN3t1S/)
[PreProcessedData](https://http://bit.ly/2hN3t1S/)

List of pre-processing method to implement: 
Cluster analysis[day of week, part of day, movement speed, chaotic/ordered] 
Dimensionality reduction [linear mapping]
  
List of ML techniques to implement

 - Anomaly detection  
 - Structured prediction  
 - Reinforcement learning
 - Supervised learning

# List of NNs 
  

 1. CNN convolutional neural network, context recognition [https://colah.github.io/posts/2014-07-Conv-Nets-Modular/][https://colah.github.io/posts/2014-07-Understanding-Convolutions/]   
 2. LSTM Long Short Term Memory network, context recognition [https://colah.github.io/posts/2015-08-Understanding-LSTMs/]
 3. GAN Generative Adversarial Networks [http://www.kdnuggets.com/2017/01/generative-adversarial-networks-hot-topic-machine-learning.html]

  
# Visualization for 
[https://colah.github.io/posts/2015-01-Visualizing-Representations/]

 - CV 
 - LinkedIn profile
 - Article for social media
 - Training process visualization 
 - Dataset visualization 
 - Working trained NN visualization
 - Illustration of the architecture with 

# Supervised learning - convolutional neural network CNN
> Point is to pretrain CNN on pre-labeled data for context activity recognition.

# Structured prediction - generative adversarial network GAN
> GANs solve a problem by training two separate networks with competitive goals.One network produces prediction for walking route  (generative) another network distinguishes between the real path and the generated route (adversarial).
The concept it to train these network competitively, so that after some time, neither network can make further progress against the other. Or the generator becomes so effective that the adversarial network can not distinguish between real and synthetic solutions, even with unlimited time and substantial resources.
GANs will be used to draw walking route, given an desirable category and a random rule:
“draw me a evening walking route, and it can’t be one of the paths that I walked before.”

# Anomaly detection - long short term memory network LSTM 
> One of the appeals of RNNs is the idea that they might be able to connect previous information to the present task, such as using previous walking route might inform the understanding of the future destination.
Sometimes, we only need to look at recent information to perform the present task. For example, consider a movement model trying to predict the next destination based on the previous ones. If we are trying to predict the last point in stable sequence we don’t need any further context – it’s pretty obvious the next point is going to be as usual. In such cases, where the gap between the relevant information and the place that it’s needed is small, RNNs can learn to use the past information.
Unfortunately, as that gap grows, RNNs become unable to learn to connect the information.
Long Short Term Memory networks – usually just called “LSTMs” – are a special kind of RNN, capable of learning long-term dependencies. They work tremendously well on a large variety of problems, and are now widely used.
LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn!
